{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Enunciado\n",
        "\n",
        "Utilizar otro dataset y\n",
        "poner en práctica\n",
        "la generación de\n",
        "secuencias con las\n",
        "estrategias presentadas."
      ],
      "metadata": {
        "id": "xwqt4c29FIxs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5Zi0seFFEGC",
        "outputId": "2a3a93cb-5915-4b1a-cbd0-cc5830e7f493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Texto cargado con 1115394 caracteres.\n"
          ]
        }
      ],
      "source": [
        "# Configuración inicial\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow.keras.layers as layers\n",
        "\n",
        "# Cargar un dataset de texto\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f'Texto cargado con {len(text)} caracteres.')\n",
        "\n",
        "# Procesamiento de texto\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# Parámetros del modelo\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Definir el Modelo LSTM para Generación de Texto\n",
        "El modelo se basa en LSTM y genera texto carácter a carácter."
      ],
      "metadata": {
        "id": "pHxuNslMFS-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construcción del modelo\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Embedding(vocab_size, embedding_dim, input_length=seq_length),  # input_length se define aquí para evitar problemas\n",
        "    layers.LSTM(rnn_units, return_sequences=True, recurrent_initializer='glorot_uniform'),\n",
        "    layers.Dense(vocab_size)\n",
        "])\n",
        "\n",
        "# Compila el modelo nuevamente\n",
        "model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEjEvO5hFVVV",
        "outputId": "f48d9fe5-8363-4b70-bd85-173326b33c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Funciones para Estrategias de Generación\n",
        "Aquí te muestro cómo implementar Greedy Search, Beam Search y Muestreo con Temperatura.\n",
        "\n",
        "## Greedy Search"
      ],
      "metadata": {
        "id": "GWhOCkRpF9rK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-tmd3LIIGEgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_greedy(model, start_string, num_generate=100):\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predicted_id = tf.argmax(predictions[0, -1]).numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n"
      ],
      "metadata": {
        "id": "gWHwL8ucGAOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beam Search\n",
        "\n"
      ],
      "metadata": {
        "id": "DDDoeGVeGC95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_beam_search(model, start_string, num_generate=100, beam_width=3):\n",
        "    sequences = [[list(start_string), 1.0]]\n",
        "    for _ in range(num_generate):\n",
        "        all_candidates = []\n",
        "        for seq, score in sequences:\n",
        "            input_eval = tf.expand_dims([char2idx[s] for s in seq], 0)\n",
        "            predictions = model(input_eval)\n",
        "            top_k = tf.math.top_k(predictions[0, -1], k=beam_width)\n",
        "            for i in range(beam_width):\n",
        "                candidate = seq + [idx2char[top_k.indices[i].numpy()]]\n",
        "                candidate_score = score * top_k.values[i].numpy()\n",
        "                all_candidates.append([candidate, candidate_score])\n",
        "        sequences = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)[:beam_width]\n",
        "\n",
        "    return ''.join(sequences[0][0])\n"
      ],
      "metadata": {
        "id": "0g1ndyQwGIg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Muestreo Aleatorio con Temperatura"
      ],
      "metadata": {
        "id": "ZsMd0UDsFYzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_temperature(model, start_string, num_generate=100, temperature=1.0):\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0) / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n"
      ],
      "metadata": {
        "id": "W7zQm-QPGPMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento del modelo"
      ],
      "metadata": {
        "id": "u0mQdMFUGTnC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aGzyIkwuG___"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Forma de la entrada:\", input_example.shape)\n",
        "    print(\"Forma del target:\", target_example.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfbF741WHBfi",
        "outputId": "0478f0a3-f97f-4032-860d-52fa692cdf03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma de la entrada: (64, 100)\n",
            "Forma del target: (64, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración de parámetros para el entrenamiento\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "# Preparación del dataset\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Configuración de los checkpoints\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")  # Añadimos .weights.h5 al final del nombre\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)\n",
        "\n",
        "# Crear el directorio de checkpoints si no existe\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "EPOCHS = 50\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "-PUTKBqRGX1w",
        "outputId": "156a1d8c-0c98-4eab-f79b-699801732ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input 0 of layer \"lstm_1\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (64, 64, 100, 256)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-40b19139fbaf>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Entrenamiento del modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_last_axis_squeeze\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    187\u001b[0m                     \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                     \u001b[0;34m\"is incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"lstm_1\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (64, 64, 100, 256)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las librerías necesarias\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow.keras.layers as layers\n",
        "\n",
        "# Descargar y cargar un dataset de texto\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')[:100000]  # Usar una muestra de los primeros 100,000 caracteres\n",
        "print(f'Texto cargado con {len(text)} caracteres.')\n",
        "\n",
        "# Procesamiento de texto\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# Parámetros del modelo\n",
        "seq_length = 100\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# Preparación del dataset\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "BATCH_SIZE =  64 # Reducido para acelerar el entrenamiento\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Construcción del modelo\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 512  # Reducido para un modelo más pequeño\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
        "    layers.LSTM(rnn_units, return_sequences=True, recurrent_initializer='glorot_uniform'),\n",
        "    layers.Dense(vocab_size)\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "# Configura los checkpoints\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "EPOCHS = 30  # Reducido para más rapidez\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "\n",
        "# Funciones para generación de texto\n",
        "def generate_text_greedy(model, start_string, num_generate=100):\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predicted_id = tf.argmax(predictions[0, -1]).numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "def generate_text_beam_search(model, start_string, num_generate=100, beam_width=3):\n",
        "    sequences = [[list(start_string), 1.0]]\n",
        "    for _ in range(num_generate):\n",
        "        all_candidates = []\n",
        "        for seq, score in sequences:\n",
        "            input_eval = tf.expand_dims([char2idx[s] for s in seq], 0)\n",
        "            predictions = model(input_eval)\n",
        "            top_k = tf.math.top_k(predictions[0, -1], k=beam_width)\n",
        "            for i in range(beam_width):\n",
        "                candidate = seq + [idx2char[top_k.indices[i].numpy()]]\n",
        "                candidate_score = score * top_k.values[i].numpy()\n",
        "                all_candidates.append([candidate, candidate_score])\n",
        "        sequences = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)[:beam_width]\n",
        "\n",
        "    return ''.join(sequences[0][0])\n",
        "\n",
        "def generate_text_temperature(model, start_string, num_generate=100, temperature=0.5):\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0) / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "# Generación de texto con diferentes técnicas\n",
        "print(\"Greedy Search:\")\n",
        "print(generate_text_greedy(model, start_string=\"ROMEO: \", num_generate=200))\n",
        "\n",
        "print(\"\\nBeam Search:\")\n",
        "print(generate_text_beam_search(model, start_string=\"ROMEO: \", num_generate=200, beam_width=3))\n",
        "\n",
        "print(\"\\nMuestreo Aleatorio con Temperatura:\")\n",
        "print(generate_text_temperature(model, start_string=\"ROMEO: \", num_generate=200, temperature=0.5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaOpEPAkJBc0",
        "outputId": "8b69bd94-e696-47ab-ce64-4c0fbc0247a6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Texto cargado con 100000 caracteres.\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2s/step - loss: 3.7762\n",
            "Epoch 2/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - loss: 3.1854\n",
            "Epoch 3/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - loss: 2.8271\n",
            "Epoch 4/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 3s/step - loss: 2.5348\n",
            "Epoch 5/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - loss: 2.3869\n",
            "Epoch 6/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - loss: 2.2806\n",
            "Epoch 7/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - loss: 2.1928\n",
            "Epoch 8/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 2.1231\n",
            "Epoch 9/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - loss: 2.0567\n",
            "Epoch 10/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - loss: 2.0029\n",
            "Epoch 11/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 1.9622\n",
            "Epoch 12/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - loss: 1.9166\n",
            "Epoch 13/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - loss: 1.8766\n",
            "Epoch 14/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - loss: 1.8458\n",
            "Epoch 15/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 1.7943\n",
            "Epoch 16/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 1.7855\n",
            "Epoch 17/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - loss: 1.7393\n",
            "Epoch 18/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 1.7126\n",
            "Epoch 19/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 2s/step - loss: 1.6937\n",
            "Epoch 20/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - loss: 1.6690\n",
            "Epoch 21/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - loss: 1.6364\n",
            "Epoch 22/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 1.6194\n",
            "Epoch 23/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 1.5884\n",
            "Epoch 24/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 1.5619\n",
            "Epoch 25/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 1.5446\n",
            "Epoch 26/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - loss: 1.5280\n",
            "Epoch 27/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - loss: 1.5020\n",
            "Epoch 28/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - loss: 1.4831\n",
            "Epoch 29/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - loss: 1.4761\n",
            "Epoch 30/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 1.4506\n",
            "Greedy Search:\n",
            "ROMEO: I the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
            "\n",
            "Beam Search:\n",
            "ROMEO: that,\n",
            "They, that what they?\n",
            "\n",
            "CORIOLANUS:\n",
            "They, that you, that that they?\n",
            "\n",
            "CORIOLANUS:\n",
            "They, that you, that that they?\n",
            "\n",
            "CORIOLANUS:\n",
            "They, that you, that that they?\n",
            "\n",
            "CORIOLANUS:\n",
            "They, that you, that the\n",
            "\n",
            "Muestreo Aleatorio con Temperatura:\n",
            "ROMEO: IUShe thitharan, t t t th the t thinthe anwinoure thathe fithathithirenis thare t t thareathathithary t anore the th the the are t thalllary thathacitres are tourachar thie the oure aren outhe t thize\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las librerías necesarias\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow.keras.layers as layers\n",
        "\n",
        "# Descargar y cargar un dataset de texto\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')[:100000]  # Usar una muestra de los primeros 100,000 caracteres\n",
        "print(f'Texto cargado con {len(text)} caracteres.')\n",
        "\n",
        "# Procesamiento de texto\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# Parámetros del modelo\n",
        "seq_length = 100\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# Preparación del dataset\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Construcción del modelo\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 512\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
        "    layers.LSTM(rnn_units, return_sequences=True, recurrent_initializer='glorot_uniform'),\n",
        "    layers.Dense(vocab_size)\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "# Configura los checkpoints\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "EPOCHS = 30\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "\n",
        "# Funciones para generación de texto\n",
        "def generate_text_greedy(model, start_string, num_generate=100):\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predicted_id = tf.argmax(predictions[0, -1]).numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "def generate_text_beam_search(model, start_string, num_generate=100, beam_width=5):  # Aumentar beam_width\n",
        "    sequences = [[list(start_string), 1.0]]\n",
        "    for _ in range(num_generate):\n",
        "        all_candidates = []\n",
        "        for seq, score in sequences:\n",
        "            input_eval = tf.expand_dims([char2idx[s] for s in seq], 0)\n",
        "            predictions = model(input_eval)\n",
        "            top_k = tf.math.top_k(predictions[0, -1], k=beam_width)\n",
        "            for i in range(beam_width):\n",
        "                candidate = seq + [idx2char[top_k.indices[i].numpy()]]\n",
        "                candidate_score = score * top_k.values[i].numpy()\n",
        "                all_candidates.append([candidate, candidate_score])\n",
        "        sequences = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)[:beam_width]\n",
        "\n",
        "    return ''.join(sequences[0][0])\n",
        "\n",
        "def generate_text_temperature(model, start_string, num_generate=100, temperature=0.8):  # Ajustar temperatura\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0) / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "# Generación de texto con diferentes técnicas\n",
        "print(\"Greedy Search:\")\n",
        "print(generate_text_greedy(model, start_string=\"ROMEO: \", num_generate=200))\n",
        "\n",
        "print(\"\\nBeam Search:\")\n",
        "print(generate_text_beam_search(model, start_string=\"ROMEO: \", num_generate=200, beam_width=5))\n",
        "\n",
        "print(\"\\nMuestreo Aleatorio con Temperatura:\")\n",
        "print(generate_text_temperature(model, start_string=\"ROMEO: \", num_generate=200, temperature=0.8))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbYc5fydZfzT",
        "outputId": "6eacb884-8386-4fee-f977-597405222205"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto cargado con 100000 caracteres.\n",
            "Epoch 1/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 2s/step - loss: 3.7246\n",
            "Epoch 2/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - loss: 3.1185\n",
            "Epoch 3/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - loss: 2.6990\n",
            "Epoch 4/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 2.4505\n",
            "Epoch 5/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 2.3169\n",
            "Epoch 6/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - loss: 2.2270\n",
            "Epoch 7/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - loss: 2.1470\n",
            "Epoch 8/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 2.0785\n",
            "Epoch 9/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - loss: 2.0093\n",
            "Epoch 10/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - loss: 1.9591\n",
            "Epoch 11/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 1.9198\n",
            "Epoch 12/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - loss: 1.8764\n",
            "Epoch 13/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 1.8425\n",
            "Epoch 14/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - loss: 1.8053\n",
            "Epoch 15/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 1.7686\n",
            "Epoch 16/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - loss: 1.7431\n",
            "Epoch 17/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - loss: 1.7074\n",
            "Epoch 18/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 1.6975\n",
            "Epoch 19/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 1.6538\n",
            "Epoch 20/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - loss: 1.6368\n",
            "Epoch 21/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - loss: 1.6073\n",
            "Epoch 22/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 1.5969\n",
            "Epoch 23/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 1.5715\n",
            "Epoch 24/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 1.5505\n",
            "Epoch 25/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - loss: 1.5341\n",
            "Epoch 26/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - loss: 1.5125\n",
            "Epoch 27/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - loss: 1.5028\n",
            "Epoch 28/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - loss: 1.4798\n",
            "Epoch 29/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 1.4480\n",
            "Epoch 30/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - loss: 1.4329\n",
            "Greedy Search:\n",
            "ROMEO: I the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
            "\n",
            "Beam Search:\n",
            "ROMEO: that, they, that than that that that, that that than that that that, that that that, that that that, that that that, that that that, that that that, that that that, that that that, that that that that\n",
            "\n",
            "Muestreo Aleatorio con Temperatura:\n",
            "ROMEO: 'NUS:\n",
            "ked as t.\n",
            "B&finon acour'se be cl tines: m\n",
            "Tinonistwupas wen ath t t y thave.\n",
            "Mar h thus\n",
            "\n",
            "bly ar o.\n",
            "\n",
            "\n",
            "OLENit wharenonore t whed be COn whe tiby he t thie coure t, hire the ou t ar thath thinr of \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las librerías necesarias\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow.keras.layers as layers\n",
        "\n",
        "# Descargar y cargar un dataset de texto\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')[:200000]  # Usar una muestra más grande de 200,000 caracteres si es posible\n",
        "print(f'Texto cargado con {len(text)} caracteres.')\n",
        "\n",
        "# Procesamiento de texto\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# Parámetros del modelo\n",
        "seq_length = 100\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# Preparación del dataset\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Construcción del modelo\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 512\n",
        "rnn_units = 1024\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
        "    layers.LSTM(rnn_units, return_sequences=True, recurrent_initializer='glorot_uniform'),\n",
        "    layers.Dense(vocab_size)\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "# Configura los checkpoints\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "EPOCHS = 50  # Aumentar el número de épocas\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "\n",
        "# Funciones para generación de texto\n",
        "def generate_text_greedy(model, start_string, num_generate=200):\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predicted_id = tf.argmax(predictions[0, -1]).numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "def generate_text_beam_search(model, start_string, num_generate=200, beam_width=5):\n",
        "    sequences = [[list(start_string), 1.0]]\n",
        "    for _ in range(num_generate):\n",
        "        all_candidates = []\n",
        "        for seq, score in sequences:\n",
        "            input_eval = tf.expand_dims([char2idx[s] for s in seq], 0)\n",
        "            predictions = model(input_eval)\n",
        "            top_k = tf.math.top_k(predictions[0, -1], k=beam_width)\n",
        "            for i in range(beam_width):\n",
        "                candidate = seq + [idx2char[top_k.indices[i].numpy()]]\n",
        "                candidate_score = score * top_k.values[i].numpy()\n",
        "                all_candidates.append([candidate, candidate_score])\n",
        "        sequences = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)[:beam_width]\n",
        "\n",
        "    return ''.join(sequences[0][0])\n",
        "\n",
        "def generate_text_temperature(model, start_string, num_generate=200, temperature=0.8):\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0) / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "# Generación de texto con diferentes técnicas\n",
        "print(\"Greedy Search:\")\n",
        "print(generate_text_greedy(model, start_string=\"ROMEO: \", num_generate=200))\n",
        "\n",
        "print(\"\\nBeam Search:\")\n",
        "print(generate_text_beam_search(model, start_string=\"ROMEO: \", num_generate=200, beam_width=5))\n",
        "\n",
        "print(\"\\nMuestreo Aleatorio con Temperatura:\")\n",
        "print(generate_text_temperature(model, start_string=\"ROMEO: \", num_generate=200, temperature=0.8))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7QvibeZgdr-",
        "outputId": "f9029a11-86c6-4047-e7b5-c6534e4b90e7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto cargado con 200000 caracteres.\n",
            "Epoch 1/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 9s/step - loss: 3.5344\n",
            "Epoch 2/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 9s/step - loss: 2.5126\n",
            "Epoch 3/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 9s/step - loss: 2.2070\n",
            "Epoch 4/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 9s/step - loss: 2.0238\n",
            "Epoch 5/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 9s/step - loss: 1.8838\n",
            "Epoch 6/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 9s/step - loss: 1.7798\n",
            "Epoch 7/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 9s/step - loss: 1.6936\n",
            "Epoch 8/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 9s/step - loss: 1.6231\n",
            "Epoch 9/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 9s/step - loss: 1.5546\n",
            "Epoch 10/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 9s/step - loss: 1.4989\n",
            "Epoch 11/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 9s/step - loss: 1.4531\n",
            "Epoch 12/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 9s/step - loss: 1.4051\n",
            "Epoch 13/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 9s/step - loss: 1.3487\n",
            "Epoch 14/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 9s/step - loss: 1.2959\n",
            "Epoch 15/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 9s/step - loss: 1.2580\n",
            "Epoch 16/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 9s/step - loss: 1.2097\n",
            "Epoch 17/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 9s/step - loss: 1.1609\n",
            "Epoch 18/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 9s/step - loss: 1.1083\n",
            "Epoch 19/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 9s/step - loss: 1.0510\n",
            "Epoch 20/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 9s/step - loss: 0.9964\n",
            "Epoch 21/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 9s/step - loss: 0.9317\n",
            "Epoch 22/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 9s/step - loss: 0.8745\n",
            "Epoch 23/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 9s/step - loss: 0.8006\n",
            "Epoch 24/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 9s/step - loss: 0.7313\n",
            "Epoch 25/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 9s/step - loss: 0.6633\n",
            "Epoch 26/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 9s/step - loss: 0.5876\n",
            "Epoch 27/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 9s/step - loss: 0.5141\n",
            "Epoch 28/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 9s/step - loss: 0.4503\n",
            "Epoch 29/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 9s/step - loss: 0.3937\n",
            "Epoch 30/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 9s/step - loss: 0.3370\n",
            "Epoch 31/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 9s/step - loss: 0.2876\n",
            "Epoch 32/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 9s/step - loss: 0.2461\n",
            "Epoch 33/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 9s/step - loss: 0.2134\n",
            "Epoch 34/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 9s/step - loss: 0.1806\n",
            "Epoch 35/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 9s/step - loss: 0.1536\n",
            "Epoch 36/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 9s/step - loss: 0.1344\n",
            "Epoch 37/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 9s/step - loss: 0.1162\n",
            "Epoch 38/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 9s/step - loss: 0.1031\n",
            "Epoch 39/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 9s/step - loss: 0.0944\n",
            "Epoch 40/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 9s/step - loss: 0.0869\n",
            "Epoch 41/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 9s/step - loss: 0.0811\n",
            "Epoch 42/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 9s/step - loss: 0.0761\n",
            "Epoch 43/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 9s/step - loss: 0.0733\n",
            "Epoch 44/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 9s/step - loss: 0.0696\n",
            "Epoch 45/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 9s/step - loss: 0.0684\n",
            "Epoch 46/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 9s/step - loss: 0.0666\n",
            "Epoch 47/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 9s/step - loss: 0.0650\n",
            "Epoch 48/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 9s/step - loss: 0.0641\n",
            "Epoch 49/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 9s/step - loss: 0.0627\n",
            "Epoch 50/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 9s/step - loss: 0.0615\n",
            "Greedy Search:\n",
            "ROMEO: I the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
            "\n",
            "Beam Search:\n",
            "ROMEO: you shall hear him.\n",
            "\n",
            "CORIOLANUS:\n",
            "Hail, lords!!\n",
            "\n",
            "CORIOLANUS::\n",
            "You?\n",
            "\n",
            "MENENIUS:\n",
            "Is thich them makes their voices.\n",
            "\n",
            "BRUTUS:\n",
            "There's no more.\n",
            "\n",
            "CORIOLANUS::\n",
            "What is that?\n",
            "\n",
            "MENENIUS:\n",
            "Hear me, peace!\n",
            "\n",
            "MENENIU\n",
            "\n",
            "Muestreo Aleatorio con Temperatura:\n",
            "ROMEO: In m ale meache te t and t an twno h here s as, t wan n by thyofo.\n",
            "Whistch maw watist f akif he As mo izaisth mo t haist s\n",
            "CENI me wir's s ang amod wnd what this are I t band ble,\n",
            "\n",
            "Asthes\n",
            "Astheye se m\n"
          ]
        }
      ]
    }
  ]
}